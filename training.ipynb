{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d267c5c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "893fb328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction margin: the only parameter to set. Recommended: margin in {5, 10, 15, 20} (aka 0.5, 1, 1.5, 2 seconds)\n",
    "margin = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df02c22",
   "metadata": {},
   "source": [
    "## Import libraries and define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3375203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import floor\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6134da76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 2\n"
     ]
    }
   ],
   "source": [
    "# If you have more than one GPU in your system, the GPU with the lowest ID will be selected by default.\n",
    "# https://www.tensorflow.org/guide/gpu#using_a_single_gpu_on_a_multi-gpu_system\n",
    "print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c34a05dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = lambda l: sum(l) / len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf3b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_perf(model_name, seed, columns_name, training_columns, params, params_idx, \n",
    "                   history, best_cost, best_thr, all_cost, all_thr, perf):\n",
    "    f_path = results_path + model_name + \"-\" + columns_name + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
    "    to_serialize = (training_columns, params, history, best_cost, best_thr, all_cost, all_thr, perf)\n",
    "    with open(f_path, \"wb\") as file:\n",
    "        pickle.dump(to_serialize, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b3b931",
   "metadata": {},
   "source": [
    "#### Set up global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0d48c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.4f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19b1f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = \"results_\" + str(margin) + \"/\"\n",
    "models_path = \"models_\" + str(margin) + \"/\"\n",
    "threshold_path = \"threshold_\" + str(margin) + \"/val_\"\n",
    "seeds = [100, 200, 300]\n",
    "\n",
    "def set_determinism(seed):\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c472a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 50\n",
    "epochs = 1000\n",
    "batch_size = 128\n",
    "learning_rate = 0.0005\n",
    "validation_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2945999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w5_features_no_diff = [\n",
    " 'Gz_mean_w5',\n",
    " 'Ax_mean_w5',\n",
    " 'Ay_mean_w5',\n",
    " 'Gz_std_w5',\n",
    " 'Ax_std_w5',\n",
    " 'Ay_std_w5',\n",
    " 'Gz_min_w5',\n",
    " 'Ax_min_w5',\n",
    " 'Ay_min_w5',\n",
    " 'Gz_max_w5',\n",
    " 'Ax_max_w5',\n",
    " 'Ay_max_w5'\n",
    "]\n",
    "\n",
    "w5_features_diff = [\n",
    " 'differencing_Gz_mean_w5',\n",
    " 'differencing_Ax_mean_w5',\n",
    " 'differencing_Ay_mean_w5',\n",
    " 'differencing_Gz_std_w5',\n",
    " 'differencing_Ax_std_w5',\n",
    " 'differencing_Ay_std_w5',\n",
    " 'differencing_Gz_min_w5',\n",
    " 'differencing_Ax_min_w5',\n",
    " 'differencing_Ay_min_w5',\n",
    " 'differencing_Gz_max_w5',\n",
    " 'differencing_Ax_max_w5',\n",
    " 'differencing_Ay_max_w5',\n",
    "]\n",
    "\n",
    "w10_features_no_diff = [\n",
    " 'Gz_mean_w10',\n",
    " 'Ax_mean_w10',\n",
    " 'Ay_mean_w10',\n",
    " 'Gz_std_w10',\n",
    " 'Ax_std_w10',\n",
    " 'Ay_std_w10',\n",
    " 'Gz_min_w10',\n",
    " 'Ax_min_w10',\n",
    " 'Ay_min_w10',\n",
    " 'Gz_max_w10',\n",
    " 'Ax_max_w10',\n",
    " 'Ay_max_w10'\n",
    "]\n",
    "\n",
    "w10_features_diff = [\n",
    " 'differencing_Gz_mean_w10',\n",
    " 'differencing_Ax_mean_w10',\n",
    " 'differencing_Ay_mean_w10',\n",
    " 'differencing_Gz_std_w10',\n",
    " 'differencing_Ax_std_w10',\n",
    " 'differencing_Ay_std_w10',\n",
    " 'differencing_Gz_min_w10',\n",
    " 'differencing_Ax_min_w10',\n",
    " 'differencing_Ay_min_w10',\n",
    " 'differencing_Gz_max_w10',\n",
    " 'differencing_Ax_max_w10',\n",
    " 'differencing_Ay_max_w10'\n",
    "]\n",
    "\n",
    "w15_features_no_diff = [\n",
    " 'Gz_mean_w15',\n",
    " 'Ax_mean_w15',\n",
    " 'Ay_mean_w15',\n",
    " 'Gz_std_w15',\n",
    " 'Ax_std_w15',\n",
    " 'Ay_std_w15',\n",
    " 'Gz_min_w15',\n",
    " 'Ax_min_w15',\n",
    " 'Ay_min_w15',\n",
    " 'Gz_max_w15',\n",
    " 'Ax_max_w15',\n",
    " 'Ay_max_w15'\n",
    "]\n",
    "\n",
    "w15_features_diff = [\n",
    " 'differencing_Gz_mean_w15',\n",
    " 'differencing_Ax_mean_w15',\n",
    " 'differencing_Ay_mean_w15',\n",
    " 'differencing_Gz_std_w15',\n",
    " 'differencing_Ax_std_w15',\n",
    " 'differencing_Ay_std_w15',\n",
    " 'differencing_Gz_min_w15',\n",
    " 'differencing_Ax_min_w15',\n",
    " 'differencing_Ay_min_w15',\n",
    " 'differencing_Gz_max_w15',\n",
    " 'differencing_Ax_max_w15',\n",
    " 'differencing_Ay_max_w15'\n",
    "]\n",
    "\n",
    "w20_features_no_diff = [\n",
    " 'Gz_mean_w20',\n",
    " 'Ax_mean_w20',\n",
    " 'Ay_mean_w20',\n",
    " 'Gz_std_w20',\n",
    " 'Ax_std_w20',\n",
    " 'Ay_std_w20',\n",
    " 'Gz_min_w20',\n",
    " 'Ax_min_w20',\n",
    " 'Ay_min_w20',\n",
    " 'Gz_max_w20',\n",
    " 'Ax_max_w20',\n",
    " 'Ay_max_w20'\n",
    "]\n",
    "\n",
    "w20_features_diff = [\n",
    " 'differencing_Gz_mean_w20',\n",
    " 'differencing_Ax_mean_w20',\n",
    " 'differencing_Ay_mean_w20',\n",
    " 'differencing_Gz_std_w20',\n",
    " 'differencing_Ax_std_w20',\n",
    " 'differencing_Ay_std_w20',\n",
    " 'differencing_Gz_min_w20',\n",
    " 'differencing_Ax_min_w20',\n",
    " 'differencing_Ay_min_w20',\n",
    " 'differencing_Gz_max_w20',\n",
    " 'differencing_Ax_max_w20',\n",
    " 'differencing_Ay_max_w20',\n",
    "]\n",
    "\n",
    "features = {\n",
    "    \"all_features\": w5_features_no_diff + w10_features_no_diff + w15_features_no_diff + w20_features_no_diff + w5_features_diff + w10_features_diff + w15_features_diff + w20_features_diff + ['label'], \n",
    "    \"w5_features\": w5_features_no_diff + w5_features_diff + ['label'], \n",
    "    \"w10_features\": w10_features_no_diff + w10_features_diff + ['label'], \n",
    "    \"w15_features\": w15_features_no_diff + w15_features_diff + ['label'], \n",
    "    \"w20_features\": w20_features_no_diff + w20_features_diff + ['label'], \n",
    "    \"no_diff_features\": w5_features_no_diff + w10_features_no_diff + w15_features_no_diff + w20_features_no_diff + ['label'], \n",
    "    \"diff_features\": w5_features_diff + w10_features_diff + w15_features_diff + w20_features_diff + ['label']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2be90c9",
   "metadata": {},
   "source": [
    "#### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bdea77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train/training_\" + str(margin) + \".csv\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "967ad76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33330 rows in the dataset\n",
      "Index(['Gz', 'Ax', 'Ay', 'Gz_diff', 'Ax_diff', 'Ay_diff', 'Gz_mean_w5',\n",
      "       'Ax_mean_w5', 'Ay_mean_w5', 'Gz_std_w5',\n",
      "       ...\n",
      "       'differencing_Ay_std_w20', 'differencing_Gz_min_w20',\n",
      "       'differencing_Ax_min_w20', 'differencing_Ay_min_w20',\n",
      "       'differencing_Gz_max_w20', 'differencing_Ax_max_w20',\n",
      "       'differencing_Ay_max_w20', 'orient_discr', 'POSy_discr', 'label'],\n",
      "      dtype='object', length=114)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gz</th>\n",
       "      <th>Ax</th>\n",
       "      <th>Ay</th>\n",
       "      <th>Gz_diff</th>\n",
       "      <th>Ax_diff</th>\n",
       "      <th>Ay_diff</th>\n",
       "      <th>Gz_mean_w5</th>\n",
       "      <th>Ax_mean_w5</th>\n",
       "      <th>Ay_mean_w5</th>\n",
       "      <th>Gz_std_w5</th>\n",
       "      <th>...</th>\n",
       "      <th>differencing_Ay_std_w20</th>\n",
       "      <th>differencing_Gz_min_w20</th>\n",
       "      <th>differencing_Ax_min_w20</th>\n",
       "      <th>differencing_Ay_min_w20</th>\n",
       "      <th>differencing_Gz_max_w20</th>\n",
       "      <th>differencing_Ax_max_w20</th>\n",
       "      <th>differencing_Ay_max_w20</th>\n",
       "      <th>orient_discr</th>\n",
       "      <th>POSy_discr</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>33330.0000</td>\n",
       "      <td>33330.0000</td>\n",
       "      <td>33330.0000</td>\n",
       "      <td>33330.0000</td>\n",
       "      <td>33330.0000</td>\n",
       "      <td>33330.0000</td>\n",
       "      <td>33330.0000</td>\n",
       "      <td>33330.0000</td>\n",
       "      <td>33330.0000</td>\n",
       "      <td>33330.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>33330.0000</td>\n",
       "      <td>33330.0000</td>\n",
       "      <td>33330.0000</td>\n",
       "      <td>33330.0000</td>\n",
       "      <td>33330.0000</td>\n",
       "      <td>33330.0000</td>\n",
       "      <td>33330.0000</td>\n",
       "      <td>33330.0000</td>\n",
       "      <td>33330.0000</td>\n",
       "      <td>33330.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0071</td>\n",
       "      <td>25.0268</td>\n",
       "      <td>693.7214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9003</td>\n",
       "      <td>3.6446</td>\n",
       "      <td>486.4557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.5910</td>\n",
       "      <td>-9.4163</td>\n",
       "      <td>-6.6125</td>\n",
       "      <td>-8.9902</td>\n",
       "      <td>-8.4056</td>\n",
       "      <td>-6.2725</td>\n",
       "      <td>-3.1130</td>\n",
       "      <td>-8.1356</td>\n",
       "      <td>-4.8702</td>\n",
       "      <td>-0.9338</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.0655</td>\n",
       "      <td>-3.9112</td>\n",
       "      <td>-11.6159</td>\n",
       "      <td>-7.0120</td>\n",
       "      <td>-5.2183</td>\n",
       "      <td>-3.2827</td>\n",
       "      <td>-3.8541</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>20.1000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.5174</td>\n",
       "      <td>-0.3436</td>\n",
       "      <td>-0.5260</td>\n",
       "      <td>-0.2134</td>\n",
       "      <td>-0.4330</td>\n",
       "      <td>-0.4971</td>\n",
       "      <td>-0.5244</td>\n",
       "      <td>-0.3965</td>\n",
       "      <td>-0.6088</td>\n",
       "      <td>-0.6870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5777</td>\n",
       "      <td>-0.4783</td>\n",
       "      <td>-0.4118</td>\n",
       "      <td>-0.5316</td>\n",
       "      <td>-0.3886</td>\n",
       "      <td>-0.6152</td>\n",
       "      <td>-0.6249</td>\n",
       "      <td>-1.0000</td>\n",
       "      <td>21.1000</td>\n",
       "      <td>297.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.2785</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>-0.0226</td>\n",
       "      <td>0.0175</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>-0.2717</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>-0.0462</td>\n",
       "      <td>-0.4343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0641</td>\n",
       "      <td>0.0813</td>\n",
       "      <td>0.0572</td>\n",
       "      <td>0.0735</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>-0.1841</td>\n",
       "      <td>-0.1103</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>25.6000</td>\n",
       "      <td>603.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.6446</td>\n",
       "      <td>0.5491</td>\n",
       "      <td>0.5568</td>\n",
       "      <td>0.2488</td>\n",
       "      <td>0.4305</td>\n",
       "      <td>0.4885</td>\n",
       "      <td>0.6691</td>\n",
       "      <td>0.6927</td>\n",
       "      <td>0.5950</td>\n",
       "      <td>0.3711</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5177</td>\n",
       "      <td>0.3880</td>\n",
       "      <td>0.4509</td>\n",
       "      <td>0.6111</td>\n",
       "      <td>0.5340</td>\n",
       "      <td>0.4085</td>\n",
       "      <td>0.5402</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>28.9000</td>\n",
       "      <td>1003.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.6170</td>\n",
       "      <td>4.2651</td>\n",
       "      <td>6.0183</td>\n",
       "      <td>9.0084</td>\n",
       "      <td>8.7169</td>\n",
       "      <td>7.9005</td>\n",
       "      <td>2.0682</td>\n",
       "      <td>2.3163</td>\n",
       "      <td>4.9736</td>\n",
       "      <td>7.5768</td>\n",
       "      <td>...</td>\n",
       "      <td>5.6167</td>\n",
       "      <td>4.0408</td>\n",
       "      <td>4.2086</td>\n",
       "      <td>3.8691</td>\n",
       "      <td>3.9355</td>\n",
       "      <td>5.3217</td>\n",
       "      <td>5.9881</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>29.2000</td>\n",
       "      <td>2079.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Gz         Ax         Ay    Gz_diff    Ax_diff    Ay_diff  \\\n",
       "count 33330.0000 33330.0000 33330.0000 33330.0000 33330.0000 33330.0000   \n",
       "mean     -0.0000    -0.0000     0.0000    -0.0000    -0.0000    -0.0000   \n",
       "std       1.0000     1.0000     1.0000     1.0000     1.0000     1.0000   \n",
       "min      -3.5910    -9.4163    -6.6125    -8.9902    -8.4056    -6.2725   \n",
       "25%      -0.5174    -0.3436    -0.5260    -0.2134    -0.4330    -0.4971   \n",
       "50%      -0.2785     0.0458    -0.0226     0.0175     0.0045     0.0034   \n",
       "75%       0.6446     0.5491     0.5568     0.2488     0.4305     0.4885   \n",
       "max       2.6170     4.2651     6.0183     9.0084     8.7169     7.9005   \n",
       "\n",
       "       Gz_mean_w5  Ax_mean_w5  Ay_mean_w5  Gz_std_w5  ...  \\\n",
       "count  33330.0000  33330.0000  33330.0000 33330.0000  ...   \n",
       "mean      -0.0000      0.0000     -0.0000    -0.0000  ...   \n",
       "std        1.0000      1.0000      1.0000     1.0000  ...   \n",
       "min       -3.1130     -8.1356     -4.8702    -0.9338  ...   \n",
       "25%       -0.5244     -0.3965     -0.6088    -0.6870  ...   \n",
       "50%       -0.2717      0.0523     -0.0462    -0.4343  ...   \n",
       "75%        0.6691      0.6927      0.5950     0.3711  ...   \n",
       "max        2.0682      2.3163      4.9736     7.5768  ...   \n",
       "\n",
       "       differencing_Ay_std_w20  differencing_Gz_min_w20  \\\n",
       "count               33330.0000               33330.0000   \n",
       "mean                    0.0000                   0.0000   \n",
       "std                     1.0000                   1.0000   \n",
       "min                    -4.0655                  -3.9112   \n",
       "25%                    -0.5777                  -0.4783   \n",
       "50%                    -0.0641                   0.0813   \n",
       "75%                     0.5177                   0.3880   \n",
       "max                     5.6167                   4.0408   \n",
       "\n",
       "       differencing_Ax_min_w20  differencing_Ay_min_w20  \\\n",
       "count               33330.0000               33330.0000   \n",
       "mean                   -0.0000                  -0.0000   \n",
       "std                     1.0000                   1.0000   \n",
       "min                   -11.6159                  -7.0120   \n",
       "25%                    -0.4118                  -0.5316   \n",
       "50%                     0.0572                   0.0735   \n",
       "75%                     0.4509                   0.6111   \n",
       "max                     4.2086                   3.8691   \n",
       "\n",
       "       differencing_Gz_max_w20  differencing_Ax_max_w20  \\\n",
       "count               33330.0000               33330.0000   \n",
       "mean                   -0.0000                   0.0000   \n",
       "std                     1.0000                   1.0000   \n",
       "min                    -5.2183                  -3.2827   \n",
       "25%                    -0.3886                  -0.6152   \n",
       "50%                     0.0164                  -0.1841   \n",
       "75%                     0.5340                   0.4085   \n",
       "max                     3.9355                   5.3217   \n",
       "\n",
       "       differencing_Ay_max_w20  orient_discr  POSy_discr      label  \n",
       "count               33330.0000    33330.0000  33330.0000 33330.0000  \n",
       "mean                    0.0000       -0.0071     25.0268   693.7214  \n",
       "std                     1.0000        0.9003      3.6446   486.4557  \n",
       "min                    -3.8541       -1.0000     20.1000     0.0000  \n",
       "25%                    -0.6249       -1.0000     21.1000   297.0000  \n",
       "50%                    -0.1103        0.0000     25.6000   603.5000  \n",
       "75%                     0.5402        1.0000     28.9000  1003.0000  \n",
       "max                     5.9881        1.0000     29.2000  2079.0000  \n",
       "\n",
       "[8 rows x 114 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df), \"rows in the dataset\")\n",
    "print(df.columns)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5765f47",
   "metadata": {},
   "source": [
    "# Machine learning: training phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279553f",
   "metadata": {},
   "source": [
    "## Dataset preprocessing for machine learning models\n",
    "\n",
    "In this section, RUL labels are converted to binary labels (`0/1`, namely `not_fault/fault`) in order to perform classification instead of regression.\n",
    "\n",
    "For the `AutoEncoder` model, the dataset is partitioned such that the training set does not contain faults or samples which anticipate a fault. In other words, each sample must be compliant with the `good_samples_thr` threshold.\n",
    "\n",
    "We basically need an entire section of dataset where faults are not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d19cf5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset_for_ml_model(df, training_columns, split_size=0.75, as_list=False, ae=False):\n",
    "    dfs = []\n",
    "    df_main = df[training_columns]\n",
    "    fault_indexes = df_main.index[df_main[\"label\"] == 0].tolist() # list of indexes representing faults\n",
    "    good_samples_thr = margin * 2\n",
    "    \n",
    "    previous = 0\n",
    "    for fi in fault_indexes:\n",
    "        dfs.append(df_main.iloc[previous:fi+1, :])\n",
    "        previous = fi + 1\n",
    "    \n",
    "    rnd_list = list(range(len(dfs)))\n",
    "    \n",
    "    # If split_size is 1, there will be no val/test set\n",
    "    train_size = floor(len(dfs) * split_size)\n",
    "    train_index = rnd_list[:train_size]\n",
    "    test_index = rnd_list[train_size:]\n",
    "    train_rul = []\n",
    "    test_rul = []\n",
    "    \n",
    "    if not as_list:\n",
    "        first = True\n",
    "        for ti in train_index:\n",
    "            if not ae:\n",
    "                to_concat = dfs[ti].copy()\n",
    "            else:\n",
    "                to_concat = dfs[ti][dfs[ti][\"label\"] >= good_samples_thr].copy()\n",
    "            if first:\n",
    "                training_set = to_concat\n",
    "                first = False\n",
    "            else:\n",
    "                training_set = pd.concat([training_set, to_concat])\n",
    "\n",
    "        first = True\n",
    "        for ti in test_index:\n",
    "            to_concat = dfs[ti].copy()\n",
    "            if first:\n",
    "                test_set = to_concat\n",
    "                first = False\n",
    "            else:\n",
    "                test_set = pd.concat([test_set, to_concat])\n",
    "        \n",
    "        train_rul = training_set['label'].tolist()\n",
    "        if split_size < 1:\n",
    "            test_rul = test_set['label'].tolist()\n",
    "        \n",
    "        training_set['label'] = (training_set['label'] >= margin).map({True: 1, False: 0})\n",
    "        if split_size < 1:\n",
    "            test_set['label'] = (test_set['label'] >= margin).map({True: 1, False: 0})\n",
    "\n",
    "        training_set = training_set.to_numpy()\n",
    "        if split_size < 1:\n",
    "            test_set = test_set.to_numpy()\n",
    "        \n",
    "    else:\n",
    "        first = True\n",
    "        for ti in train_index:\n",
    "            if not ae:\n",
    "                to_concat = dfs[ti].copy()\n",
    "            else:\n",
    "                to_concat = dfs[ti][dfs[ti][\"label\"] >= good_samples_thr].copy()\n",
    "            if first:\n",
    "                training_set = [to_concat]\n",
    "                first = False\n",
    "            else:\n",
    "                training_set.append(to_concat)\n",
    "                \n",
    "        first = True\n",
    "        for ti in test_index:\n",
    "            to_concat = dfs[ti].copy()\n",
    "            if first:\n",
    "                test_set = [to_concat]\n",
    "                first = False\n",
    "            else:\n",
    "                test_set.append(to_concat)\n",
    "        \n",
    "        for t in training_set:\n",
    "            train_rul = train_rul + t['label'].tolist()\n",
    "            t['label'] = (t['label'] >= margin).map({True: 1, False: 0})\n",
    "        if split_size < 1:\n",
    "            for t in test_set:\n",
    "                test_rul = test_rul + t['label'].tolist()\n",
    "                t['label'] = (t['label'] >= margin).map({True: 1, False: 0})\n",
    "    if split_size < 1:\n",
    "        return training_set, test_set\n",
    "    return training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c131072",
   "metadata": {},
   "source": [
    "## Cost model for threshold optimization and performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d58986c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_perf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7551194",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_FP = 0.2\n",
    "BASE_FN = 1\n",
    "\n",
    "def false_positive_cost(i, is_fault, fault_found):\n",
    "    return BASE_FP\n",
    "\n",
    "def false_negative_cost(i, is_fault, fault_found):\n",
    "    if not fault_found:\n",
    "        for j in range(1, margin + 1):\n",
    "            if i + j < is_fault.shape[0] and not is_fault[i + j] or i + j >= is_fault.shape[0]:\n",
    "                return (margin + 1 - j) * BASE_FN\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1359dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_optimization(signal, rul, start, end, n_steps):\n",
    "    best_cost = sys.maxsize\n",
    "    best_thr = -1\n",
    "    all_cost = []\n",
    "    all_thr = []\n",
    "    is_fault = (rul == 0)\n",
    "    \n",
    "    for thr in np.linspace(start, end, n_steps):\n",
    "        tmp_cost = 0\n",
    "        fault_found = False\n",
    "        for i in range(signal.shape[0]):\n",
    "            if is_fault[i] and signal[i] >= thr:\n",
    "                fault_found = True\n",
    "            if not is_fault[i]:\n",
    "                fault_found = False\n",
    "            if not is_fault[i] and signal[i] >= thr:\n",
    "                tmp_cost += false_positive_cost(i, is_fault, fault_found)\n",
    "            elif is_fault[i] and signal[i] <= thr:\n",
    "                tmp_cost += false_negative_cost(i, is_fault, fault_found)\n",
    "        if tmp_cost < best_cost:\n",
    "            best_thr = thr\n",
    "            best_cost = tmp_cost\n",
    "        all_cost.append(tmp_cost)\n",
    "        all_thr.append(thr)\n",
    "\n",
    "    return best_cost, best_thr, all_cost, all_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5eacfd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_threshold(signal, thr, rul):\n",
    "\n",
    "    plt.plot(signal, alpha=0.5)\n",
    "    plt.plot(range(len(signal)), [thr] * len(signal))\n",
    "\n",
    "    ranges = []\n",
    "    signal_values = []\n",
    "    for i in range(len(rul)):\n",
    "        if rul[i] == 0:\n",
    "            ranges.append(i)\n",
    "            signal_values.append(signal[i])\n",
    "\n",
    "    plt.scatter(ranges, signal_values, color=\"red\", s=10)\n",
    "    \n",
    "    plt.ylabel('Alarm signal intensity')\n",
    "    plt.xlabel('Time')\n",
    "    plt.legend(['Alarm signal', \"Threshold\", 'Anomalies'], loc='upper right')\n",
    "    plt.show()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14a8cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_evaluation(signal, thr, rul):\n",
    "    fp, fn, tp, tot_p = 0, 0, 0, 0\n",
    "    cost = 0\n",
    "    alarm = (signal >= thr)\n",
    "    anticipation = []\n",
    "    is_fault = (rul == 0)\n",
    "    \n",
    "    fault_found = False\n",
    "    for i in range(len(rul)):\n",
    "        if i > 0 and is_fault[i] and not is_fault[i - 1]:\n",
    "            tot_p += 1\n",
    "            start = i\n",
    "        if is_fault[i] and not fault_found and alarm[i]:\n",
    "            tp += 1\n",
    "            fault_found = True\n",
    "            anticipation.append((margin - 1) - (i - start))\n",
    "        if (i < len(rul) - 1 and is_fault[i] and not is_fault[i + 1] and not fault_found) or (i == len(rul) - 1 and not fault_found):\n",
    "            fn += 1 \n",
    "        if is_fault[i] and signal[i] <= thr:\n",
    "            cost += false_negative_cost(i, is_fault, fault_found)\n",
    "        if not is_fault[i]:\n",
    "            fault_found = False\n",
    "            if alarm[i]:\n",
    "                fp += 1\n",
    "                cost += false_positive_cost(i, is_fault, fault_found)\n",
    "        \n",
    "    tot_a = sum(anticipation) / 10\n",
    "    if sum(anticipation) > 0:\n",
    "        mean_a = mean(anticipation) / 10\n",
    "    else:\n",
    "        mean_a = 0\n",
    "    \n",
    "    return [cost, mean_a, tp, fn, fp]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec634d",
   "metadata": {},
   "source": [
    "## Baseline: raw signal pre-anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94a954fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_raw = [\n",
    " 'Ax',\n",
    " 'Ax_diff',\n",
    " 'Ax_mean_w5',\n",
    " 'Ax_std_w5',\n",
    " 'Ax_min_w5',\n",
    " 'Ax_max_w5',\n",
    " 'Ax_mean_w10',\n",
    " 'Ax_std_w10',\n",
    " 'Ax_min_w10',\n",
    " 'Ax_max_w10',\n",
    " 'Ax_mean_w15',\n",
    " 'Ax_std_w15',\n",
    " 'Ax_min_w15',\n",
    " 'Ax_max_w15',\n",
    " 'Ax_mean_w20',\n",
    " 'Ax_std_w20',\n",
    " 'Ax_min_w20',\n",
    " 'Ax_max_w20',\n",
    " 'differencing_Ax',\n",
    " 'differencing_Ax_diff',\n",
    " 'differencing_Ax_mean_w5',\n",
    " 'differencing_Ax_std_w5',\n",
    " 'differencing_Ax_min_w5',\n",
    " 'differencing_Ax_max_w5',\n",
    " 'differencing_Ax_mean_w10',\n",
    " 'differencing_Ax_std_w10',\n",
    " 'differencing_Ax_min_w10',\n",
    " 'differencing_Ax_max_w10',\n",
    " 'differencing_Ax_mean_w15',\n",
    " 'differencing_Ax_std_w15',\n",
    " 'differencing_Ax_min_w15',\n",
    " 'differencing_Ax_max_w15',\n",
    " 'differencing_Ax_mean_w20',\n",
    " 'differencing_Ax_std_w20',\n",
    " 'differencing_Ax_min_w20',\n",
    " 'differencing_Ax_max_w20',\n",
    "]\n",
    "\n",
    "for seed in seeds:\n",
    "    for feature in features_raw:\n",
    "        \n",
    "        set_determinism(seed)\n",
    "        \n",
    "        training_columns = [feature, \"label\"]\n",
    "        _, validation_set_raw = build_dataset_for_ml_model(df, training_columns=training_columns)\n",
    "        \n",
    "        val_raw_signal, val_raw_rul = -validation_set_raw[:, 0], validation_set_raw[:, -1]\n",
    "        \n",
    "        best_cost_raw, best_thr_raw, all_cost_raw, all_thr_raw = threshold_optimization(val_raw_signal, val_raw_rul, start=0, end=val_raw_signal.max(), n_steps=200)\n",
    "        \n",
    "        f_path = threshold_path + \"raw_signal\" + \"-\" + feature + \"-s\" + str(seed) + \"-p0\"\n",
    "        to_serialize = (val_raw_signal, best_thr_raw, val_raw_rul)\n",
    "        with open(f_path, \"wb\") as file:\n",
    "            pickle.dump(to_serialize, file)\n",
    "        \n",
    "        perf_raw = performance_evaluation(val_raw_signal, best_thr_raw, val_raw_rul)\n",
    "        all_perf.append([\"raw_signal\", seed, feature, {}] + perf_raw)\n",
    "        \n",
    "        serialize_perf(\"raw_signal\", seed=seed, columns_name=feature, training_columns=training_columns, \n",
    "                       params={}, params_idx=0, history=None, best_cost=best_cost_raw, best_thr=best_thr_raw, \n",
    "                       all_cost=all_cost_raw, all_thr=all_thr_raw, perf=perf_raw)\n",
    "        \n",
    "        # No model saving here: there is just the threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691254d9",
   "metadata": {},
   "source": [
    "## Pre-anomaly detection with AutoEncoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20a0a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(input_size, hidden):\n",
    "    input_shape = (input_size, )\n",
    "    ae_x = keras.Input(shape=input_shape, dtype='float32')\n",
    "    x = ae_x\n",
    "    for h in hidden:\n",
    "        x = layers.Dense(h, activation='relu')(x)\n",
    "    ae_y = layers.Dense(input_size, activation='linear')(x)\n",
    "    ae = keras.Model(ae_x, ae_y)   \n",
    "    \n",
    "    return ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc59e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_ae = [{\"hidden_ae\": [16, 8, 2, 8, 16]}, \n",
    "             {\"hidden_ae\": [64, 24, 9, 24, 64]}, \n",
    "             {\"hidden_ae\": [128, 56, 18, 56, 128]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1138942f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221/221 [==============================] - 0s 576us/step\n",
      "INFO:tensorflow:Assets written to: ram://5fd31bd7-3e7d-44a0-b8af-b1e22f326a83/assets\n",
      "INFO:tensorflow:Assets written to: models_30/autoencoder-all_features-s100-p0/assets\n",
      "221/221 [==============================] - 0s 573us/step\n",
      "INFO:tensorflow:Assets written to: ram://40005caf-1faf-4948-a0a5-8523a1167393/assets\n",
      "INFO:tensorflow:Assets written to: models_30/autoencoder-all_features-s100-p1/assets\n"
     ]
    }
   ],
   "source": [
    "for seed in seeds:\n",
    "    for columns in features:\n",
    "        for params_idx, params in enumerate(params_ae):\n",
    "\n",
    "            set_determinism(seed)\n",
    "\n",
    "            training_set_ae, validation_set_ae = build_dataset_for_ml_model(df, training_columns=features[columns], ae=True)\n",
    "\n",
    "            train_cols_ae = training_set_ae.shape[1] - 1\n",
    "            ae = build_autoencoder(input_size=train_cols_ae, hidden=params[\"hidden_ae\"])\n",
    "            ae.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                       loss='mse')\n",
    "            cb_ae = [callbacks.EarlyStopping(patience=patience, restore_best_weights=True)]\n",
    "            history_ae = ae.fit(training_set_ae[:, :-1], training_set_ae[:, :-1], validation_split=validation_split,\n",
    "                                callbacks=cb_ae, batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "            \n",
    "            preds_ae = ae.predict(validation_set_ae[:, :-1])\n",
    "            \n",
    "            signal_ae = pd.Series(data=np.sum(np.square(preds_ae - validation_set_ae[:, :-1]), axis=1))\n",
    "            rul_ae = validation_set_ae[:, -1]\n",
    "\n",
    "            best_cost_ae, best_thr_ae, all_cost_ae, all_thr_ae = threshold_optimization(signal_ae, rul_ae, start=0, end=signal_ae.max(), n_steps=200)\n",
    "                \n",
    "            f_path = threshold_path + \"autoencoder\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
    "            to_serialize = (signal_ae, best_thr_ae, rul_ae)\n",
    "            with open(f_path, \"wb\") as file:\n",
    "                pickle.dump(to_serialize, file)\n",
    "            \n",
    "            perf_ae = performance_evaluation(signal_ae, best_thr_ae, rul_ae)\n",
    "            all_perf.append([\"autoencoder\", seed, columns, params] + perf_ae)\n",
    "            \n",
    "            serialize_perf(\"autoencoder\", seed=seed, columns_name=columns, training_columns=features[columns], \n",
    "                           params=params, params_idx=params_idx, history=history_ae, best_cost=best_cost_ae, best_thr=best_thr_ae, \n",
    "                           all_cost=all_cost_ae, all_thr=all_thr_ae, perf=perf_ae)\n",
    "            \n",
    "            ae.save(models_path + \"autoencoder\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d123add4",
   "metadata": {},
   "source": [
    "## RUL estimation with Dense Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a98d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(input_size, hidden):\n",
    "    input_shape = (input_size,)\n",
    "    model_in = keras.Input(shape=input_shape, dtype='float32')\n",
    "    x = model_in\n",
    "    for h in hidden:\n",
    "        x = layers.Dense(h, activation='relu')(x)\n",
    "    model_out = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = keras.Model(model_in, model_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f39e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_mlp = [{\"hidden_mlp\": []},\n",
    "              {\"hidden_mlp\": [32]},\n",
    "              {\"hidden_mlp\": [64, 32]}, \n",
    "              {\"hidden_mlp\": [128, 64, 32]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ad43d6",
   "metadata": {},
   "source": [
    "**Class weights** are useful when you have an **unbalanced dataset** and you want to improve single-label classification results. With class weights, you can weight more the samples belonging to the rarest class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e0e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in seeds:\n",
    "    for columns in features:\n",
    "        for params_idx, params in enumerate(params_mlp):\n",
    "\n",
    "            set_determinism(seed)\n",
    "            \n",
    "            training_set_mlp, validation_set_mlp = build_dataset_for_ml_model(df, training_columns=features[columns])\n",
    "            counts_mlp = pd.Series(training_set_mlp[:, -1]).value_counts(normalize=True)\n",
    "            class_weight_mlp = {0: 1/counts_mlp[0], 1: 1/counts_mlp[1]}\n",
    "            \n",
    "            input_size_mlp = training_set_mlp.shape[1] - 1\n",
    "            mlp = build_classifier(input_size=input_size_mlp, hidden=params[\"hidden_mlp\"])\n",
    "            mlp.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss='binary_crossentropy')\n",
    "            cb_mlp = [callbacks.EarlyStopping(patience=patience, restore_best_weights=True)]\n",
    "            history_mlp = mlp.fit(training_set_mlp[:, :-1], training_set_mlp[:, -1], validation_split=validation_split,\n",
    "                                  callbacks=cb_mlp, class_weight=class_weight_mlp,\n",
    "                                  batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "                \n",
    "            preds_mlp = mlp.predict(validation_set_mlp[:, :-1]).ravel()\n",
    "            \n",
    "            signal_mlp = pd.Series(data=(1 - preds_mlp))\n",
    "            rul_mlp = validation_set_mlp[:, -1]\n",
    "\n",
    "            best_cost_mlp, best_thr_mlp, all_cost_mlp, all_thr_mlp = threshold_optimization(signal_mlp, rul_mlp, start=0, end=signal_mlp.max(), n_steps=200)\n",
    "            \n",
    "            f_path = threshold_path + \"mlp\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
    "            to_serialize = (signal_mlp, best_thr_mlp, rul_mlp)\n",
    "            with open(f_path, \"wb\") as file:\n",
    "                pickle.dump(to_serialize, file)\n",
    "            \n",
    "            perf_mlp = performance_evaluation(signal_mlp, best_thr_mlp, rul_mlp)\n",
    "            all_perf.append([\"mlp\", seed, columns, params] + perf_mlp)\n",
    "            \n",
    "            serialize_perf(\"mlp\", seed=seed, columns_name=columns, training_columns=features[columns], \n",
    "                           params=params, params_idx=params_idx, history=history_mlp, best_cost=best_cost_mlp, best_thr=best_thr_mlp, \n",
    "                           all_cost=all_cost_mlp, all_thr=all_thr_mlp, perf=perf_mlp)\n",
    "            \n",
    "            mlp.save(models_path + \"mlp\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed1a1bb",
   "metadata": {},
   "source": [
    "## RUL estimation with Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd3879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_2D(data, w_len, stride=1):\n",
    "    # Get shifted tables\n",
    "    m = len(data)\n",
    "    lt = [data.iloc[i:m-w_len+i+1:stride, :].values for i in range(w_len)]\n",
    "    # Reshape to add a new axis\n",
    "    s = lt[0].shape\n",
    "    for i in range(w_len):\n",
    "        lt[i] = lt[i].reshape(s[0], 1, s[1])\n",
    "    # Concatenate\n",
    "    wdata = np.concatenate(lt, axis=1)\n",
    "    return wdata\n",
    "\n",
    "\n",
    "def sliding_window_by_fault(data, cols, w_len, stride=1):\n",
    "    l_w, l_r = [], []\n",
    "    for gdata in data:\n",
    "        # Apply a sliding window\n",
    "        tmp_w = sliding_window_2D(gdata[cols], w_len, stride)\n",
    "        # Build the RUL vector\n",
    "        tmp_r = gdata['label'].iloc[w_len-1::stride]\n",
    "        # Store everything\n",
    "        l_w.append(tmp_w)\n",
    "        l_r.append(tmp_r)\n",
    "    res_w = np.concatenate(l_w)\n",
    "    res_r = np.concatenate(l_r)\n",
    "    return res_w, res_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b19381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_regressor(input_size, filters, kernel_size, hidden, w_len):\n",
    "    input_shape = (w_len, input_size)\n",
    "    model_in = keras.Input(shape=input_shape, dtype='float32')\n",
    "    model_out = layers.Conv1D(filters, kernel_size=kernel_size, \n",
    "                              activation='relu')(model_in)\n",
    "    model_out = layers.Flatten()(model_out)\n",
    "    for h in hidden:\n",
    "        model_out = layers.Dense(h, activation='relu')(model_out)\n",
    "    model_out = layers.Dense(1, activation='sigmoid')(model_out)\n",
    "    model = keras.Model(model_in, model_out)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b75ef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_cnn = [{\"filters\": 1, \"kernel_size\": 3, \"hidden\": [32], \"w_len\": 5},\n",
    "              {\"filters\": 4, \"kernel_size\": 3, \"hidden\": [32], \"w_len\": 5},\n",
    "              {\"filters\": 1, \"kernel_size\": 5, \"hidden\": [32], \"w_len\": 5},\n",
    "              {\"filters\": 4, \"kernel_size\": 5, \"hidden\": [32], \"w_len\": 5},\n",
    "              {\"filters\": 4, \"kernel_size\": 5, \"hidden\": [64, 32], \"w_len\": 5},\n",
    "              {\"filters\": 1, \"kernel_size\": 3, \"hidden\": [32], \"w_len\": 10},\n",
    "              {\"filters\": 4, \"kernel_size\": 3, \"hidden\": [32], \"w_len\": 10},\n",
    "              {\"filters\": 1, \"kernel_size\": 5, \"hidden\": [32], \"w_len\": 10},\n",
    "              {\"filters\": 4, \"kernel_size\": 5, \"hidden\": [32], \"w_len\": 10},\n",
    "              {\"filters\": 4, \"kernel_size\": 5, \"hidden\": [64, 32], \"w_len\": 10},\n",
    "              {\"filters\": 4, \"kernel_size\": 7, \"hidden\": [128, 64, 32], \"w_len\": 10}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8535c7aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for seed in seeds:\n",
    "    for columns in features:\n",
    "        for params_idx, params in enumerate(params_cnn):\n",
    "            \n",
    "            set_determinism(seed)\n",
    "            \n",
    "            traning_set_cnn, validation_set_cnn = build_dataset_for_ml_model(df, training_columns=features[columns], as_list=True)\n",
    "            tr_sw, tr_sw_r = sliding_window_by_fault(traning_set_cnn, features[columns], params[\"w_len\"])\n",
    "            val_sw, val_sw_r = sliding_window_by_fault(validation_set_cnn, features[columns], params[\"w_len\"])\n",
    "            counts_cnn = pd.Series(tr_sw_r).value_counts(normalize=True)\n",
    "            class_weight_cnn = {0: 1/counts_cnn[0], 1: 1/counts_cnn[1]}\n",
    "\n",
    "            input_size_cnn = tr_sw[0].shape[1]\n",
    "            cnn = build_cnn_regressor(input_size=input_size_cnn, filters=params[\"filters\"],\n",
    "                                      kernel_size=params[\"kernel_size\"], hidden=params[\"hidden\"], w_len=params[\"w_len\"])\n",
    "            cnn.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                        loss='binary_crossentropy')\n",
    "            cb_cnn = [callbacks.EarlyStopping(patience=patience, restore_best_weights=True)]\n",
    "            history_cnn = cnn.fit(tr_sw, tr_sw_r, validation_split=validation_split,\n",
    "                                  callbacks=cb_cnn,\n",
    "                                  class_weight=class_weight_cnn,\n",
    "                                  batch_size=batch_size, epochs=epochs, verbose=0)\n",
    "            \n",
    "            preds_cnn = cnn.predict(val_sw).ravel()\n",
    "            \n",
    "            signal_cnn = pd.Series(data=(1 - preds_cnn))\n",
    "            rul_cnn = val_sw_r\n",
    "\n",
    "            best_cost_cnn, best_thr_cnn, all_cost_cnn, all_thr_cnn = threshold_optimization(signal_cnn, rul_cnn, start=0, end=signal_cnn.max(), n_steps=200)\n",
    "            \n",
    "            f_path = threshold_path + \"conv_nn\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
    "            to_serialize = (signal_cnn, best_thr_cnn, rul_cnn)\n",
    "            with open(f_path, \"wb\") as file:\n",
    "                pickle.dump(to_serialize, file)\n",
    "            \n",
    "            perf_cnn = performance_evaluation(signal_cnn, best_thr_cnn, rul_cnn)\n",
    "            all_perf.append([\"conv_nn\", seed, columns, params] + perf_cnn)\n",
    "            \n",
    "            serialize_perf(\"conv_nn\", seed=seed, columns_name=columns, training_columns=features[columns], \n",
    "                           params=params, params_idx=params_idx, history=history_cnn, best_cost=best_cost_cnn, best_thr=best_thr_cnn, \n",
    "                           all_cost=all_cost_cnn, all_thr=all_thr_cnn, perf=perf_cnn)\n",
    "            \n",
    "            cnn.save(models_path + \"conv_nn\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a9cffd",
   "metadata": {},
   "source": [
    "## RUL estimation with Recurrent Neural Networks (LSTM and GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346264c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_3D(X, y, time_steps):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X)-time_steps):\n",
    "        v = X[i:i+time_steps, :]\n",
    "        Xs.append(v)\n",
    "        ys.append(y[i+time_steps])\n",
    "    return np.array(Xs), np.array(ys).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76c2161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_cl(units, m, X_train, lr):\n",
    "    model = keras.Sequential()\n",
    "    model.add(m(units=units, return_sequences=True,\n",
    "                input_shape=[X_train.shape[1], X_train.shape[2]]))\n",
    "    model.add(m(units=units))\n",
    "    model.add(keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=lr))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bbcf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rnn = [{\"time_steps\": 5, \"units\": 64},\n",
    "              {\"time_steps\": 5, \"units\": 128},\n",
    "              {\"time_steps\": 10, \"units\": 64},\n",
    "              {\"time_steps\": 10, \"units\": 128}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e5782",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in seeds:\n",
    "    for columns in features:\n",
    "        for params_idx, params in enumerate(params_rnn):\n",
    "            \n",
    "            set_determinism(seed)\n",
    "\n",
    "            traning_set_rnn, validation_set_rnn = build_dataset_for_ml_model(df, training_columns=features[columns])\n",
    "            X_train_rnn, y_train_rnn = create_dataset_3D(traning_set_rnn[:, :-1], \n",
    "                                                         traning_set_rnn[:, -1], \n",
    "                                                         params[\"time_steps\"])\n",
    "            X_val_rnn, y_val_rnn = create_dataset_3D(validation_set_rnn[:, :-1], \n",
    "                                                     validation_set_rnn[:, -1],   \n",
    "                                                     params[\"time_steps\"])\n",
    "            counts_rnn = pd.Series(tr_sw_r).value_counts(normalize=True)\n",
    "            class_weight_rnn = {0: 1/counts_rnn[0], 1: 1/counts_rnn[1]}\n",
    "            \n",
    "            model_gru = create_model_cl(params[\"units\"], keras.layers.GRU, X_train_rnn, learning_rate)\n",
    "            early_stop = [callbacks.EarlyStopping(patience=patience, restore_best_weights=True)]\n",
    "            history_gru = model_gru.fit(X_train_rnn, y_train_rnn, epochs=epochs,\n",
    "                                        class_weight=class_weight_rnn,\n",
    "                                        validation_split=validation_split, batch_size=batch_size,\n",
    "                                        shuffle=False, callbacks=early_stop)\n",
    "            \n",
    "            model_lstm = create_model_cl(params[\"units\"], keras.layers.LSTM, X_train_rnn, learning_rate)\n",
    "            early_stop = [callbacks.EarlyStopping(patience=patience, restore_best_weights=True)]\n",
    "            history_lstm = model_lstm.fit(X_train_rnn, y_train_rnn, epochs=epochs,\n",
    "                                          class_weight=class_weight_rnn,\n",
    "                                          validation_split=validation_split, batch_size=batch_size,\n",
    "                                          shuffle=False, callbacks=early_stop, verbose=0)\n",
    "\n",
    "            preds_lstm = model_lstm.predict(X_val_rnn).ravel()\n",
    "            preds_gru = model_gru.predict(X_val_rnn).ravel()\n",
    "            \n",
    "            signal_lstm = pd.Series(data=(1 - preds_lstm))\n",
    "            rul_lstm = y_val_rnn\n",
    "            signal_gru = pd.Series(data=(1 - preds_gru))\n",
    "            rul_gru = y_val_rnn\n",
    "            \n",
    "            best_cost_lstm, best_thr_lstm, all_cost_lstm, all_thr_lstm = threshold_optimization(signal_lstm, rul_lstm, start=0, end=signal_lstm.max(), n_steps=200)\n",
    "            best_cost_gru, best_thr_gru, all_cost_gru, all_thr_gru = threshold_optimization(signal_gru, rul_gru, start=0, end=signal_gru.max(), n_steps=200)\n",
    "\n",
    "            f_path = threshold_path + \"lstm\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
    "            to_serialize = (signal_lstm, best_thr_lstm, rul_lstm)\n",
    "            with open(f_path, \"wb\") as file:\n",
    "                pickle.dump(to_serialize, file)\n",
    "                \n",
    "            f_path = threshold_path + \"gru\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
    "            to_serialize = (signal_gru, best_thr_gru, rul_gru)\n",
    "            with open(f_path, \"wb\") as file:\n",
    "                pickle.dump(to_serialize, file)\n",
    "            \n",
    "            perf_lstm = performance_evaluation(signal_lstm, best_thr_lstm, rul_lstm)\n",
    "            perf_gru = performance_evaluation(signal_gru, best_thr_gru, rul_gru)\n",
    "            all_perf.append([\"lstm\", seed, columns, params] + perf_lstm)  \n",
    "            all_perf.append([\"gru\", seed, columns, params] + perf_gru)\n",
    "            \n",
    "            serialize_perf(\"lstm\", seed=seed, columns_name=columns, training_columns=features[columns], \n",
    "                           params=params, params_idx=params_idx, history=history_lstm, best_cost=best_cost_lstm, best_thr=best_thr_lstm, \n",
    "                           all_cost=all_cost_lstm, all_thr=all_thr_lstm, perf=perf_lstm)\n",
    "            \n",
    "            model_lstm.save(models_path + \"lstm\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx))\n",
    "            \n",
    "            serialize_perf(\"gru\", seed=seed, columns_name=columns, training_columns=features[columns], \n",
    "                           params=params, params_idx=params_idx, history=history_gru, best_cost=best_cost_gru, best_thr=best_thr_gru, \n",
    "                           all_cost=all_cost_gru, all_thr=all_thr_gru, perf=perf_gru)\n",
    "            \n",
    "            model_gru.save(models_path + \"gru\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b435c370",
   "metadata": {},
   "source": [
    "## RUL estimation with Recurrent Neural Networks (BiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b128c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_bilstm_cl(units, X_train, lr):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=units,                             \n",
    "              return_sequences=True),\n",
    "              input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(keras.layers.Bidirectional(keras.layers.LSTM(units=units)))\n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=lr))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b1f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_bilstm = [{\"time_steps\": 5, \"units\": 64},\n",
    "                 {\"time_steps\": 5, \"units\": 128},\n",
    "                 {\"time_steps\": 10, \"units\": 64},\n",
    "                 {\"time_steps\": 10, \"units\": 128}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3bf177",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in seeds:\n",
    "    for columns in features:\n",
    "        for params_idx, params in enumerate(params_bilstm):\n",
    "            \n",
    "            set_determinism(seed)\n",
    "\n",
    "            traning_set_bilstm, validation_set_bilstm = build_dataset_for_ml_model(df, training_columns=features[columns])\n",
    "            X_train_bilstm, y_train_bilstm = create_dataset_3D(traning_set_bilstm[:, :-1], \n",
    "                                                               traning_set_bilstm[:, -1], \n",
    "                                                               params[\"time_steps\"])\n",
    "            X_val_bilstm, y_val_bilstm = create_dataset_3D(validation_set_bilstm[:, :-1], \n",
    "                                                           validation_set_bilstm[:, -1],   \n",
    "                                                           params[\"time_steps\"])\n",
    "            counts_bilstm = pd.Series(tr_sw_r).value_counts(normalize=True)\n",
    "            class_weight_bilstm = {0: 1/counts_bilstm[0], 1: 1/counts_bilstm[1]}\n",
    "            model_bilstm = create_model_bilstm_cl(params[\"units\"], X_train_bilstm, learning_rate)\n",
    "            early_stop = [callbacks.EarlyStopping(patience=patience, restore_best_weights=True)]\n",
    "            history_bilstm = model_bilstm.fit(X_train_bilstm, y_train_bilstm, epochs=epochs,\n",
    "                                              class_weight=class_weight_bilstm,\n",
    "                                              validation_split=validation_split, batch_size=batch_size,\n",
    "                                              shuffle=False, callbacks=early_stop, verbose=0)\n",
    "\n",
    "            preds_bilstm = model_bilstm.predict(X_val_bilstm).ravel()\n",
    "            \n",
    "            signal_bilstm = pd.Series(data=(1 - preds_bilstm))\n",
    "            rul_bilstm = y_val_bilstm\n",
    "\n",
    "            best_cost_bilstm, best_thr_bilstm, all_cost_bilstm, all_thr_bilstm = threshold_optimization(signal_bilstm, rul_bilstm, start=0, end=signal_bilstm.max(), n_steps=200)\n",
    "            \n",
    "            f_path = threshold_path + \"bilstm\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx)\n",
    "            to_serialize = (signal_bilstm, best_thr_bilstm, rul_bilstm)\n",
    "            with open(f_path, \"wb\") as file:\n",
    "                pickle.dump(to_serialize, file)\n",
    "            \n",
    "            perf_bilstm = performance_evaluation(signal_bilstm, best_thr_bilstm, rul_bilstm)\n",
    "            all_perf.append([\"bilstm\", seed, columns, params] + perf_bilstm)\n",
    "            \n",
    "            serialize_perf(\"bilstm\", seed=seed, columns_name=columns, training_columns=features[columns], \n",
    "                           params=params, params_idx=params_idx, history=history_bilstm, best_cost=best_cost_bilstm, best_thr=best_thr_bilstm, \n",
    "                           all_cost=all_cost_bilstm, all_thr=all_thr_bilstm, perf=perf_bilstm)\n",
    "            \n",
    "            model_bilstm.save(models_path + \"bilstm\" + \"-\" + columns + \"-s\" + str(seed) + \"-p\" + str(params_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c74558",
   "metadata": {},
   "source": [
    "## Analysis over the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a33c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res = pd.DataFrame(all_perf, columns=[\"model\", \"seed\", \"columns\", \"params\", \"cost\", \"anticipation\", \"detected_faults\", \"missed_faults\", \"false_alarms\"])\n",
    "df_res.to_csv(\"training_summary_\" + str(margin) + \".csv\")\n",
    "df_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
